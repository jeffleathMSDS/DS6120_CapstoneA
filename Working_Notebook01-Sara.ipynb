{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone A - Working Notebook\n",
    "\n",
    "\n",
    "Fall 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "## Contents\n",
    "\n",
    "* <a href=\"#abstract\">Abstract</a>\n",
    "* <a href=\"#intro\">Introduction</a>\n",
    "* <a href=\"#methods\">Methods</a>\n",
    "* <a href=\"#data\">Data</a>\n",
    "* <a href=\"#results\">Results</a>\n",
    "* <a href=\"#conclude\">Conclusion</a>\n",
    "* <a href=\"#ref\">References</a>\n",
    "\n",
    "\n",
    "* <a href=\"#append\">Appendix</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"abstract\"></a>\n",
    "## Abstract\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "abcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "abcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"background\"></a>\n",
    "## Methods\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "abcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lib\"></a>\n",
    "## Libraries\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sarazaheri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Basic/Standard\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "#StatsModel\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#NLP\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pickle import load\n",
    "\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pickle import load\n",
    "\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Data\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data file\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 159571\n",
      "number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "# check the structure\n",
    "\n",
    "print(\"number of rows: {0}\".format(df.shape[0]))\n",
    "print(\"number of columns: {0}\".format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# check the structure\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the structure\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count      mean       std  min  25%  50%  75%  max\n",
       "toxic          159571.0  0.095844  0.294379  0.0  0.0  0.0  0.0  1.0\n",
       "severe_toxic   159571.0  0.009996  0.099477  0.0  0.0  0.0  0.0  1.0\n",
       "obscene        159571.0  0.052948  0.223931  0.0  0.0  0.0  0.0  1.0\n",
       "threat         159571.0  0.002996  0.054650  0.0  0.0  0.0  0.0  1.0\n",
       "insult         159571.0  0.049364  0.216627  0.0  0.0  0.0  0.0  1.0\n",
       "identity_hate  159571.0  0.008805  0.093420  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the structure\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be doing a binary classification problem (Toxic, Non-Toxic).\n",
    "Below, we will compress the features to this binary feature, and drop the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add temp field for aggregation of toxic flags\n",
    "df['temp1'] = df['toxic'] + df['severe_toxic'] + df['obscene'] + df['threat'] + df ['insult'] + df['identity_hate']\n",
    "\n",
    "# add isToxic feature\n",
    "df['istoxic'] = np.where(df['temp1'] ==0,0,1)\n",
    "\n",
    "#drop unneccessary columns\n",
    "\n",
    "df = df.drop(columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate', 'temp1'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>istoxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "   istoxic  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>istoxic</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.101679</td>\n",
       "      <td>0.302226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count      mean       std  min  25%  50%  75%  max\n",
       "istoxic  159571.0  0.101679  0.302226  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "istoxic\n",
       "0    143346\n",
       "1     16225\n",
       "Name: istoxic, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check valid unique values per column\n",
    "df.groupby('istoxic')['istoxic'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKPreprocesor(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,stopwords = None,punct = None,lower = True,strip=True):\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = stopwords or set(sw.words('english'))\n",
    "        self.punct = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "#        self.contractions = load(open('contractions.pickle','rb'))\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self,X):\n",
    "        pass\n",
    "\n",
    "    def transform(self,X):\n",
    "        return [list(self.tokenize(doc)) for doc in X]\n",
    "\n",
    "    def tokenize(self,sDocument):\n",
    "        document=sDocument\n",
    "#        doc.strip(\" \")\n",
    "        for sent in sent_tokenize(document):\n",
    "            for token,tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "                token = token.strip('#') if self.strip else token\n",
    "\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                if len(token) <= 0:\n",
    "                    continue\n",
    "\n",
    "                lemma = self.lemmatize(token,tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self,token,tag):\n",
    "        tag ={\n",
    "            'N' : wn.NOUN,\n",
    "            'V' : wn.VERB,\n",
    "            'R' : wn.ADV,\n",
    "            'J' : wn.ADJ\n",
    "        }.get(tag[0],wn.NOUN)\n",
    "        return self.lemmatizer.lemmatize(token,tag)\n",
    "\n",
    "\n",
    "def prepare_data(X,X_t):\n",
    "    preProcess = Pipeline([\n",
    "        ('NLTKpreprocess', NLTKPreprocesor()),\n",
    "        ('vectorizer', TfidfVectorizer(\n",
    "            max_df=0.90,\n",
    "            max_features=5000,\n",
    "            encoding='latin1',\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=None, lowercase=False))\n",
    "    ])\n",
    "    x_train = np.asanyarray(preProcess.fit_transform(X.values).todense())\n",
    "#    x_valid = np.asanyarray(preProcess.fit_transform(X_v.values).todense())    \n",
    "    x_test = np.asanyarray(preProcess.transform(X_t.values).todense())\n",
    "\n",
    "    return x_train,x_test,preProcess\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv(\"data/train.csv\", dtype=str) ## reading the data from csv file ## \n",
    "    clf = NLTKPreprocesor()\n",
    "    procData = clf.transform(data.loc[:, 'comment_text']) ## preprocessing the comments ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pData=np.asarray(procData)\n",
    "print(pData.shape[:])\n",
    "print(data.loc[:,'toxic'].shape[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into test section (0.1) and training section (0.9), I simplified the project from multi-label classifier to mono-label classifier just using toxic label. Later the training section is also devided to validation and training sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "X_train, X_test, y_train, y_test=train_test_split(pData, data.loc[:,'toxic'], shuffle=False, test_size=0.1)\n",
    "#trainData=data.loc[:int(Length*0.8),'comment_text'] ## Training Data\n",
    "#validData=data.loc[int(Length*0.8):int(Length*0.9),'comment_text'] ## Validation data \n",
    "#tvData=data.loc[:int(Length*0.9),'comment_text'] ## Validation data \n",
    "#testData=data.loc[int(Length*0.9):,'comment_text'] ## testing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the toxic comments from non-toxic comments for pre-analysis and descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicX_train=[]\n",
    "NonToxic_train=[]\n",
    "for i,comment in enumerate(X_train):\n",
    "    if int(y_train[i])==1:\n",
    "        toxicX_train.append(comment)\n",
    "    else:\n",
    "        NonToxic_train.append(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment length distribution after preprocessing for label and non-labeled comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalNumWords = [len(one_comment) for one_comment in X_train]\n",
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.title(\"Length Histogram\")\n",
    "plt.xlabel(\"Number of Words in a comment\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ToxicNumWords = [len(one_comment) for one_comment in toxicX_train]\n",
    "plt.hist(ToxicNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.title(\"Length histogram of Toxic Comments\")\n",
    "plt.xlabel(\"Number of Words in a comment\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "NonToxicNumWords = [len(one_comment) for one_comment in NonToxic_train]\n",
    "plt.hist(NonToxicNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.xlabel(\"Number of Words in a comment\")\n",
    "plt.title(\"Length histogram of Non-Toxic Comments\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the toxic and non-toxic comments separated numpy arrays # \n",
    "# Printing the number of nont-toxic and toxic comments #\n",
    "print (np.asarray(NonToxicNumWords).sum())\n",
    "ntoxic_text_train=[]\n",
    "for x in NonToxic_train:\n",
    "    x=' '.join(x)\n",
    "    ntoxic_text_train.append(x.encode('utf-8'))\n",
    "\n",
    "ntoxic_text_train=np.asarray(ntoxic_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(ToxicNumWords).sum())\n",
    "toxic_text_train=[]\n",
    "for x in toxicX_train:\n",
    "    x=' '.join(x)\n",
    "    toxic_text_train.append(x.encode('utf-8'))\n",
    "\n",
    "toxic_text_train=np.asarray(toxic_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(totalNumWords).sum())\n",
    "text_train=[]\n",
    "for x in X_train:\n",
    "    x=' '.join(x)\n",
    "    text_train.append(x.encode('utf-8'))\n",
    "\n",
    "text_test=[]\n",
    "for x in X_test:\n",
    "    x=' '.join(x)\n",
    "    text_test.append(x.encode('utf-8'))\n",
    "text_train=np.asarray(text_train)\n",
    "text_test=np.asarray(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the pie histogram of imbalanced number of (toxic, non-toxic) comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Data to plot\n",
    "labels ='Non-Toxic', 'Toxic'\n",
    "#title=\"Imbalanced classes\"\n",
    "sizes = [4774994, 397084]\n",
    "colors = ['gold', 'lightskyblue']#, 'lightcoral', 'lightskyblue']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    " \n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    " \n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing keras libraries for both tokenizing and LSTM-RNN classification\n",
    "\n",
    "Tokenizing the comments\n",
    "\n",
    "Later, I printed the most common words for both toxic and non-toxic comments separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## Results\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# Any results you write \n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GlobalMaxPool1D\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "list_text_train=[]\n",
    "L=np.asarray(text_train).tolist()\n",
    "for i in range(len(L)):\n",
    "    list_text_train.append(str(L[i]))\n",
    "    \n",
    "tokenizer.fit_on_texts(list_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "z=tokenizer.word_counts\n",
    "sorted_tokens = sorted(z.items(), key=operator.itemgetter(1))\n",
    "L=len(sorted_tokens)\n",
    "print(\"Popular words in non-labeled Comments\")\n",
    "print(L)\n",
    "for i in range(L-1,L-20,-1):\n",
    "    print(sorted_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToxicTokenizer = Tokenizer(num_words=max_features)\n",
    "list_toxic_text_train=[]\n",
    "L=np.asarray(toxic_text_train).tolist()\n",
    "for i in range(len(L)):\n",
    "    list_toxic_text_train.append(str(L[i]))\n",
    "\n",
    "ToxicTokenizer.fit_on_texts(list(np.asarray(list_toxic_text_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz=ToxicTokenizer.word_counts\n",
    "toxic_sorted_tokens = sorted(tz.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tL=len(toxic_sorted_tokens)\n",
    "#print L\n",
    "print(\"Popular words in Toxic Comments\")\n",
    "for i in range(tL-1,tL-20,-1):\n",
    "    print(toxic_sorted_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NonToxicTokenizer = Tokenizer(num_words=max_features)\n",
    "list_ntoxic_text_train=[]\n",
    "L=np.asarray(ntoxic_text_train).tolist()\n",
    "for i in range(len(L)):\n",
    "    list_ntoxic_text_train.append(str(L[i]))\n",
    "\n",
    "NonToxicTokenizer.fit_on_texts(list_ntoxic_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntz=NonToxicTokenizer.word_counts\n",
    "ntoxic_sorted_tokens = sorted(ntz.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nL = len(ntoxic_sorted_tokens)\n",
    "print( \"Popular words in Non-Toxic Comments\")\n",
    "print (L)\n",
    "for i in range(nL-1,nL-20,-1):\n",
    "    print (ntoxic_sorted_tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the phase for Solution model, LSTM-RNN (Sara) #\n",
    "Start padding the comments with fixed size of 100, chosen from comment length distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlength=100\n",
    "paddedTrain=pad_sequences(list_tokenized_train,maxlen=maxlength) # Padding the traning section\n",
    "paddedTest=pad_sequences(list_tokenized_test,maxlen=maxlength) # Padding the testing section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LSTM Classifier,\n",
    "#I have written a function for making the architecture using the given inputs for LSTM classifier #\n",
    "from LSTM_Classifier import LSTM_Classifier \n",
    "checkCLF=LSTM_Classifier(embDim=128, lstmDim=60, hidDim=50, outDim=1, maxlen=100)\n",
    "checkCLF.summary()\n",
    "Path='saved_models/weights.best.RNN.' # I saved the trained weights for extended use and transfer learning # \n",
    "checkpoint=ModelCheckpoint(filepath=Path, \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "history=checkCLF.fit(paddedTrain, y_train, validation_split=0.1,\n",
    "                epochs=epochs,\n",
    "                batch_size=256,\n",
    "                callbacks=[checkpoint], verbose=1,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestTrainPred=checkCLF.predict(paddedTrain) ## Predicting the results for test and training section using trained LSTM Classifier ##\n",
    "BestTestPred=checkCLF.predict(paddedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start measuring the metrics over the performance of our solution model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, fbeta_score  \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "#from sklearn.metrics import jaccard_similarity_score\n",
    "from functools import partial\n",
    "from collections import defaultdict \n",
    "\n",
    "METRICS = {\n",
    "    \"confusion_matrix\":confusion_matrix,\n",
    "    \"hamming_loss\": hamming_loss,\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,     \n",
    "    \"recall\": recall_score, \n",
    "    \"f1\": f1_score,\n",
    "    \"f2\": partial(fbeta_score, beta=2),\n",
    "}\n",
    "\n",
    "ytest=y_test.tolist()\n",
    "Y=[]\n",
    "for i in range(len(ytest)):\n",
    "    Y.append(int(ytest[i]))\n",
    "\n",
    "records = defaultdict(dict)\n",
    "list_classes=[]\n",
    "list_classes.append(\"toxic\")\n",
    "for metric_name,metric in METRICS.items():\n",
    "    records[metric_name][list_classes[0]]=metric(Y,BinaryTestPred[:,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(dict(records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting confusion matrix for LSTM (Solution model) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure()\n",
    "plot_confusion_matrix(records[\"confusion_matrix\"][\"toxic\"],[\"Non-Toxic\",\"Toxic\"],normalize=False,title='Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclude\"></a>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "\n",
    "## References\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"append\"></a>\n",
    "\n",
    "## Appendix\n",
    "\n",
    "<a href=\"#top\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
